{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde61eeb",
   "metadata": {},
   "source": [
    "# Tâche réalisée\n",
    "\n",
    "Ici, la tâche réalisée est la création d'un jeu de données complémentaire afin de détecter des technolectes médicaux liés.\n",
    "\n",
    "Nous avons prit l'ensemble du manuel Merck et nous avons réalisé les étapes suivantes :\n",
    "- **suppression manuelle des X premières lignes du manuel** : ces lignes correspondent au sommaire et NE DOIVENT PAS se retrouver dans le jeu de données qu'on construit (ou alors on peut les exploiter différement ?)\n",
    "- **unification des lignes du corpus, des \"pages\"** : on veut un pavé de texte découpable en phrases. Comme l'a précisé Toufik, certaines phrases sont coupées à la fin d'une page, il faut donc récupérer les informations segmentées.\n",
    "- **découpage en phrases** : une fois l'unification effectuée, on découpe notre corpus en liste de phrases avec Spacy.\n",
    "- **identification du vocabulaire** : pour chaque phrase du corpus, on effectue un tri en ne conservant que les mots qui ne sont pas présents dans la liste des mots du français. On récupère ainsi la majorité du vocabulaire médical.\n",
    "- **concaténation des phrases mal découpées à partir du vocabulaire** : pour chaque phrase, on regarde son vocabulaire. S'il est inférieur à 2, la phrase (et le vocabulaire) ne permettent pas de mettre deux technolectes en commun, on concatène donc cette phrase (et ce vocabulaire) avec la phrase (et le vocabulaire) précédent. RETIRER CE PROCEDE ? \n",
    "- **Stockage des données** : on sauvegarde notre corpus de phrases et de vocabulaires dans un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8777c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string as strii\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "sp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def writeJson(path,data):\n",
    "    with open(path,\"w\",encoding='utf-8') as f:\n",
    "        json.dump(data,f,indent=4,ensure_ascii=False)\n",
    "        \n",
    "def openJson(path):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data  \n",
    "\n",
    "def removePunct(string):\n",
    "    return string.translate(str.maketrans(strii.punctuation, ' '*len(strii.punctuation))) #map punctuation to space\n",
    "\n",
    "def tokenizer(string):\n",
    "    spacy_object = sp(string)\n",
    "    return [word.text for word in spacy_object if word.is_stop == False] #and word.pos_ != \"PUNCT\" and word.pos_ != \"NUM\"\n",
    "\n",
    "def sentenceSplit(string):\n",
    "    sp.max_length = 10500000\n",
    "    sp.add_pipe('sentencizer')\n",
    "    spacy_object = sp(string, disable = ['ner', 'parser'])\n",
    "    return [sentence.text for sentence in spacy_object.sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f55ce22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unification des pages du corpus Merck\n",
    "\n",
    "#La première étape est de supprimer manuellement le sommaire !\n",
    "#La seconde étape consiste à supprimer toutes les lignes correspondant aux noms des chapitres et aux URLs\n",
    "#Les schémas sont : \"\\d / [A-Z]*\" pour les noms de chapitres et \"http://pro.msd-france.com\" pour les URLs\n",
    "#La dernière étape consiste à joindre les lignes et à appliquer un découpage par phrases.\n",
    "\n",
    "with open(\"input/Manuel_Merck.txt\",'r',encoding='utf-8') as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    \n",
    "new_lines = []\n",
    "for line in lines:\n",
    "    if line != \"\" and not re.search(r\"\\d / [A-Z]*\",line) and not re.search(\"http://pro.msd-france.com\",line):\n",
    "        new_lines.append(line)\n",
    "\n",
    "manuelMerckSentences = sentenceSplit(\" \".join(new_lines))\n",
    "writeJson(\"output/corpusRef/manuelMerckSentences.json\",manuelMerckSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d042dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'une nouvelle liste \"manuelMerckSentences vocabulaire médical\" afin de ne conserver que les\n",
    "#technolectes médicaux au sein de chaque phrase.\n",
    "\n",
    "liste = \"input/liste.de.mots.francais.frgut.txt\"\n",
    "with open(liste,'r',encoding='utf-8') as f:\n",
    "     liste_mots = [line.rstrip('\\n').lower() for line in f]\n",
    "        \n",
    "manuelMerckSentences = openJson(\"output/corpusRef/manuelMerckSentences.json\")\n",
    "manuelMerckSentencesVocMed = []\n",
    "\n",
    "for sentence in manuelMerckSentences:\n",
    "    vocMed = []\n",
    "    for word in tokenizer(removePunct(sentence)):\n",
    "        if len(word) > 1:\n",
    "            if word.lower() not in liste_mots:\n",
    "                vocMed.append(word)\n",
    "    manuelMerckSentencesVocMed.append(vocMed)\n",
    "\n",
    "writeJson(\"output/corpusRef/manuelMerckSentences voc médical.json\",manuelMerckSentencesVocMed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47cbcffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour chaque phrase, on regarde son vocabulaire. S'il est inférieur à 2, on concatène la phrase\n",
    "#avec la phrase précédente.\n",
    "\n",
    "sentences = openJson(\"output/corpusRef/manuelMerckSentences.json\")\n",
    "vocabulaires = openJson(\"output/corpusRef/manuelMerckSentences voc médical.json\")\n",
    "\n",
    "new_sent = [sentences[0]] \n",
    "new_voc = [vocabulaires[0]]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    vocabulaires[i] = [x.strip(' ') for x in vocabulaires[i] if x !=\"\"]\n",
    "    if i == 0:\n",
    "        pass\n",
    "    if len(vocabulaires[i]) >= 2: #2 mots clefs = le minimum pour les similarités pour la suite\n",
    "        new_sent.append(sentences[i])\n",
    "        new_voc.append(vocabulaires[i])\n",
    "    else:\n",
    "        new_sent[-1] = new_sent[-1] + sentences[i]\n",
    "        new_voc[-1] = new_voc[-1] + vocabulaires[i]        \n",
    "\n",
    "writeJson(\"output/corpusRef/manuelMerckSentences.json\",new_sent)\n",
    "writeJson(\"output/corpusRef/manuelMerckSentences voc médical.json\",new_voc)\n",
    "\n",
    "#à la fin, trois problèmes repérés :\n",
    "# - quelques occurrences de \"phrases\" mal découpées\n",
    "# - certaines phrases (contenant ■ ou ❍) sont des annexes / sommaires et peuvent poser problème\n",
    "# - il y a encore du bruit dans les mots clefs, ce qui peut poser problème ?\n",
    "#   (PROBLEME si les questions et les phrases du corpus possèdent les mêmes mots clefs problématiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4239a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concaténation des phrases et de leur voc dans un csv\n",
    "\n",
    "sentences = openJson(\"output/corpusRef/manuelMerckSentences.json\")\n",
    "vocabulaires = openJson(\"output/corpusRef/manuelMerckSentences voc médical.json\")\n",
    "\n",
    "convert = [{\"id\":i,\"sentence\":sentences[i],\"vocabulary\":vocabulaires[i]} for i in range(len(sentences))]\n",
    "df = pd.DataFrame.from_records(convert)\n",
    "df.to_csv(\"output/corpusRef/MerckCorpus.csv\",sep=\"\\t\",encoding=\"utf-8\",index=False)\n",
    "\n",
    "#plus de 30 000 phrases dans ce corpus\n",
    "#peut-être faire un set ? on aperçoit des doublons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEFT2023",
   "language": "python",
   "name": "deft2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
