{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb4a9a8",
   "metadata": {},
   "source": [
    "# Résumé de la tâche\n",
    "\n",
    "On a plusieurs éléments sur lesquels travailler à ce stade : \n",
    "- un jeu de données contenant des questions et des réponses\n",
    "- un ensemble de réponses à ces questions\n",
    "- un corpus traitant de sujets médicaux\n",
    "\n",
    "Nous disposons également de plusieurs pistes / contraintes :\n",
    "- une indication du nombre de réponses possibles dans la question (systématique ?)\n",
    "- la présence ou non d'une négation (on recherche ce qui est vrai ou ce qui est faux ?)\n",
    "- le vocabulaire médical extrait de chaque question et chaque réponse\n",
    "\n",
    "# Méthodologie\n",
    "\n",
    "La première piste serait d'extraire les informations utiles des questions, que nous avons déjà mentionnées :\n",
    "- nombre de réponses possibles\n",
    "- négation ou affirmation ?\n",
    "- vocabulaire médical\n",
    "\n",
    "Une fois ces informations extraites des questions, on peut \"unifier\" le vocabulaire de la question avec celui de chaque réponse possible à la question, créant ainsi des ensembles :\n",
    "- Q1 + R1 = E1\n",
    "- Q1 + R2 = E2\n",
    "- Q1 + R3 = E3\n",
    "- Q1 + R4 = E4\n",
    "\n",
    "Cela signifie lier un terme ou plusieurs termes médicaux (celui / ceux de la question) avec d'autres termes médicaux (ceux des réponses). Pour chaque ensemble, on regarde s'il existe dans le corpus une phrase (ou autre unité) qui lie les termes de cet ensemble grâce à une mesure de similarité en Ngrammes de char_wb. Une similarité au mot ferait peu de sens dans ce corpus en vue de la singularité des termes employés.\n",
    "\n",
    "Avec les résultats obtenus, on crée un classement. On regarde ensuite si la question contient une négation et le nombre de réponses indiquées : \n",
    "- si on a 3 résultats à trouver, on prend les trois meilleurs ensembles selon les mesures de similarité\n",
    "- si on a un résultat à trouver et une négation, on prend l'ensemble avec la pire mesure de similarité\n",
    "- si on a 2 résultats et une négation, on prend les deux pires ensembles en terme de mesure de similarité\n",
    "- ...\n",
    "\n",
    "## Extraction du vocabulaire médical\n",
    "\n",
    "Pour extraire le vocabulaire médical des questions et des réponses, plutôt que de trouver un corpus spécialisé assez exhaustif pour contenir tous les tecnolectes médicaux présents dans notre jeu de données, nous effectuons le raisonnement inverse : nous retrouvons les mots médicaux en supprimant tous les mots n'appartenant pas à ce domaine. Pour chaque question et pour chaque réponse, on a ainsi des listes de tecnolectes médicaux.\n",
    "\n",
    "## Détection de la négation (pas terminée)\n",
    "\n",
    "Il existe sûrement une solution avec Spacy (POS tagging -> étiquette \"NEG\" ?).\n",
    "Attention : il peut exister des cas où la négation est présente mais n'indique pas une recherche de résultats \"faux\". \n",
    "\n",
    "## Nombre de bons résultats attendus\n",
    "\n",
    "Voir avec Toufik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6a7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports et fonctions\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import string as strii\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import pairwise_kernels\n",
    "\n",
    "import spacy\n",
    "sp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def removePunct(string):\n",
    "    return string.translate(str.maketrans(strii.punctuation, ' '*len(strii.punctuation))) #map punctuation to space\n",
    "\n",
    "def tokenizer(string):\n",
    "    spacy_object = sp(string)\n",
    "    return [word.text for word in spacy_object if word.is_stop == False] #and word.pos_ != \"PUNCT\" and word.pos_ != \"NUM\"\n",
    "\n",
    "def getKeywords(string,liste_mots,liste_keywords):\n",
    "    keywords = []\n",
    "    for word in tokenizer(removePunct(string)):\n",
    "            word = re.sub(\" *\",\"\",word)\n",
    "            if len(word) > 1:\n",
    "                if word.lower() not in liste_mots:\n",
    "                    keywords.append(word)\n",
    "                elif word.lower() in liste_keywords:\n",
    "                    keywords.append(word)\n",
    "    return keywords\n",
    "\n",
    "def cosine_similarity(vec,target,liste):\n",
    "    return pairwise_kernels(vec.transform([target]),liste,metric='cosine')\n",
    "\n",
    "def writeJson(path,data):\n",
    "    with open(path,\"w\",encoding='utf-8') as f:\n",
    "        json.dump(data,f,indent=4,ensure_ascii=False)\n",
    "        \n",
    "def openJson(path):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nous avons déjà un corpus de référence. Afin de créer un jeu de données utilisable avec ce corpus,\n",
    "#nous extrayons de chaque question / réponse son vocabulaire\n",
    "\n",
    "liste = \"input/liste.de.mots.francais.frgut.txt\"\n",
    "with open(liste,'r',encoding='utf-8') as f:\n",
    "     liste_mots = [line.rstrip('\\n').lower() for line in f]\n",
    "liste_keywords = openJson(\"output/corpusRef/keywordsMesh.json\")\n",
    "        \n",
    "df = pd.read_csv(\"../data/csv/train.csv\",delimiter=\";\")\n",
    "questions = df[\"question\"]\n",
    "ids = df[\"id\"]\n",
    "vocByQuestions = {}\n",
    "\n",
    "for i in range(len(df)):\n",
    "    vocByQuestions[ids[i]] = []\n",
    "    keywords_q = getKeywords(questions[i],liste_mots,liste_keywords)\n",
    "   \n",
    "    for element in [\"answers.\"+l for l in \"abcde\"]:\n",
    "        keywords = getKeywords(df[element][i],liste_mots,liste_keywords)\n",
    "        if len(keywords) < 1:\n",
    "            keywords = [\"NULL\"]\n",
    "                    \n",
    "        vocByQuestions[ids[i]].append(keywords_q+keywords)\n",
    "\n",
    "writeJson(\"output/similarités/vocByQuestions.json\",vocByQuestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb4750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDEM mais avec la liste de mots de Toufik\n",
    "\n",
    "liste = \"input/listeMotsFR_Auto.txt\"\n",
    "with open(liste,'r',encoding='utf-8') as f:\n",
    "     liste_mots = [line.rstrip('\\n').lower() for line in f]\n",
    "\n",
    "df = pd.read_csv(\"../data/csv/train.csv\",delimiter=\";\")\n",
    "questions = df[\"question\"]\n",
    "ids = df[\"id\"]\n",
    "vocByQuestions = {}\n",
    "\n",
    "for i in range(len(df)):\n",
    "    vocByQuestions[ids[i]] = []\n",
    "    keywords_q = getKeywords(questions[i],liste_mots,[])\n",
    "   \n",
    "    for element in [\"answers.\"+l for l in \"abcde\"]:\n",
    "        keywords = getKeywords(df[element][i],liste_mots,[])\n",
    "        if len(keywords) < 1:\n",
    "            keywords = [\"NULL\"]\n",
    "                    \n",
    "        vocByQuestions[ids[i]].append(keywords_q+keywords)\n",
    "\n",
    "writeJson(\"output/similarités/vocByQuestions_ListeToufik.json\",vocByQuestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#un gros output à la fin / par seuil\n",
    "\n",
    "dataset = openJson(\"output/similarités/vocByQuestions.json\")\n",
    "dataref = [\" \".join(liste) for liste in openJson(\"output/corpusRef/manuelMerckSentencesKeywords.json\")]\n",
    "\n",
    "results_principale = []\n",
    "results_annexe = []\n",
    "\n",
    "counter = 0\n",
    "total = len(dataset.keys())\n",
    "\n",
    "V = CountVectorizer(lowercase=True,ngram_range=(1,3),analyzer=\"char_wb\")\n",
    "X = V.fit_transform(dataref)\n",
    "print(\"Vectorization done\")\n",
    "\n",
    "for key, listes in dataset.items():\n",
    "    res_principale = {\"id\":key,\"answers\":[]}\n",
    "    res_annexe = {\"id\":key,\"nb_answers\":0}\n",
    "    \n",
    "    counter += 1\n",
    "    print(f\"{counter}/{total}\",end=\"\\r\")\n",
    "    \n",
    "    for i,question_reponse in enumerate(listes):\n",
    "        question_reponse = \" \".join(question_reponse)\n",
    "        \n",
    "        cos = list(cosine_similarity(V,question_reponse,X)[0])\n",
    "        \n",
    "        if max(cos) >= 0.7:\n",
    "            res_principale[\"answers\"].append(str(i))\n",
    "            res_annexe[\"nb_answers\"] += 1\n",
    "    \n",
    "    res_principale[\"answers\"] = \"|\".join(res_principale[\"answers\"]).replace(\"0\",\"a\").replace(\"1\",\"b\").replace(\"2\",\"c\").replace(\"3\",\"d\").replace(\"4\",\"e\")\n",
    "    \n",
    "    results_principale.append(res_principale)\n",
    "    results_annexe.append(res_annexe)\n",
    "    \n",
    "writeJson(\"output/similarités/taskPrincipale.json\",results_principale)\n",
    "writeJson(\"output/similarités/taskAnnexe.json\",results_annexe)\n",
    "\n",
    "#garder aussi la similarité\n",
    "#pas de seuil, conserver les deux meilleures seulement parmi toutes les réponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1403c43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization done\n",
      "2170/2170\r"
     ]
    }
   ],
   "source": [
    "#Un output par objet en append / par seuil\n",
    "\n",
    "dataset = openJson(\"output/similarités/vocByQuestions.json\")\n",
    "dataref = [\" \".join(liste) for liste in openJson(\"output/corpusRef/manuelMerckSentencesKeywords.json\")]\n",
    "\n",
    "V = CountVectorizer(lowercase=True,ngram_range=(1,3),analyzer=\"char_wb\")\n",
    "X = V.fit_transform(dataref)\n",
    "print(\"Vectorization done\")\n",
    "\n",
    "def writeOutputFile(path,string):\n",
    "    with open(path, 'a',encoding='utf-8') as f:\n",
    "        f.write(f\"{string}\\n\")\n",
    "        \n",
    "counter = 0\n",
    "total = len(dataset.keys())\n",
    "\n",
    "for key, listes in dataset.items():\n",
    "\n",
    "    good_res = []\n",
    "    nb_answers = 0\n",
    "    \n",
    "    counter += 1\n",
    "    print(f\"{counter}/{total}\",end=\"\\r\")\n",
    "    \n",
    "    for i,question_reponse in enumerate(listes):\n",
    "        question_reponse = \" \".join(question_reponse)\n",
    "    \n",
    "        cos = list(cosine_similarity(V,question_reponse,X)[0])\n",
    "\n",
    "        if max(cos) >= 0.9:\n",
    "            good_res.append(str(i))\n",
    "            nb_answers += 1\n",
    "    \n",
    "    good_res = \"|\".join(good_res).replace(\"0\",\"a\").replace(\"1\",\"b\").replace(\"2\",\"c\").replace(\"3\",\"d\").replace(\"4\",\"e\")\n",
    "    writeOutputFile(\"output/similarités/taskPrincipale.csv\",f\"{key};{good_res}\")\n",
    "    writeOutputFile(\"output/similarités/taskAnnexe.csv\",f\"{key};{nb_answers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba86e6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization done\n",
      "2170/2170\r"
     ]
    }
   ],
   "source": [
    "#Un output par objet en append / meilleur résultat seulement\n",
    "\n",
    "dataset = openJson(\"output/similarités/vocByQuestions.json\")\n",
    "dataref = [\" \".join(liste) for liste in openJson(\"output/corpusRef/manuelMerckSentencesKeywords.json\")]\n",
    "\n",
    "V = CountVectorizer(lowercase=True,ngram_range=(1,3),analyzer=\"char_wb\")\n",
    "X = V.fit_transform(dataref)\n",
    "print(\"Vectorization done\")\n",
    "\n",
    "def writeOutputFile(path,string):\n",
    "    with open(path, 'a',encoding='utf-8') as f:\n",
    "        f.write(f\"{string}\\n\")\n",
    "        \n",
    "counter = 0\n",
    "total = len(dataset.keys())\n",
    "\n",
    "for key, listes in dataset.items():\n",
    "    \n",
    "    good_res = []\n",
    "    old_maxi = 0\n",
    "    nb_answers = 0\n",
    "    \n",
    "    counter += 1\n",
    "    print(f\"{counter}/{total}\",end=\"\\r\")\n",
    "    \n",
    "    for i,question_reponse in enumerate(listes):\n",
    "        question_reponse = \" \".join(question_reponse)\n",
    "    \n",
    "        cos = list(cosine_similarity(V,question_reponse,X)[0])\n",
    "        maxi = max(cos)\n",
    "\n",
    "        if maxi > old_maxi:\n",
    "            old_maxi = maxi\n",
    "            good_res = [str(i)]\n",
    "            nb_answers = 1\n",
    "    \n",
    "    good_res = good_res[0].replace(\"0\",\"a\").replace(\"1\",\"b\").replace(\"2\",\"c\").replace(\"3\",\"d\").replace(\"4\",\"e\")\n",
    "    writeOutputFile(\"output/similarités/taskPrincipale.csv\",f\"{key};{good_res}\")\n",
    "    writeOutputFile(\"output/similarités/taskAnnexe.csv\",f\"{key};{nb_answers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"pomme banane\",\"pomme\",\"banane\",\"abricot\",\"abricot banane\",\"abricot pomme\"]\n",
    "b = [\"pomme banane\",\"pomme\",\"abricot\"]\n",
    "aa = ['tt symptomatique', 'tt symptomatique', '95 normale  sériques transaminases cas cellulaires hépato taux élevés', 'traitement diagnostic Diagnostic biopsie', 'ml sulfate cas kg solution 50  magnésium hypomagnésémie IM', 'acide  repas 300 cp mg dose aspirine acétylsalicylique', 'streptobacillaire pléiomorphe bacille Streptobacillus provoquée  morsure moniliformis fièvre  Gram Fièvre rat', 'SIDA   graves chimiothérapie virus immunodéprimées muqueuse parodonte héréditaires herpès immunosuppresseurs sp Candidose infections bactérie Immunosuppression personnes maladies', ' cumulatif 50 risque', 'traits visage exposées perte soleil vésicules doigts peau zones infection']\n",
    "bb = [\"fausse particules alpha noyaux hélium\"]\n",
    "\n",
    "V = CountVectorizer(lowercase=True,ngram_range=(1,3),analyzer=\"char_wb\")\n",
    "V.fit(aa+bb)\n",
    "\n",
    "for c in bb:\n",
    "    print(pairwise_kernels(V.transform([c]),V.transform(aa),metric='cosine'))\n",
    "    \n",
    "[0.4824515401296198, 0.4824515401296198, 0.7745439610488886, 0.522860777712539, 0.7125890792026981, 0.7112519095288717, 0.642633895930556, 0.7074990394260712, 0.6226045694731197, 0.7096781511019711]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEFT2023",
   "language": "python",
   "name": "deft2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
