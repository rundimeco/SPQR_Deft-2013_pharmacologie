{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc83f71",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- effectuer les similarités entre technolectes de questions en questions et de réponses en réponses.\n",
    "- faire de même entre une question et ses réponses ?\n",
    "\n",
    "\n",
    "### Ce qui a été fait\n",
    "\n",
    "J'ai essayé de faire une première classification par règles. Pour chaque question, je crée des similarités cosinus au mot entre cette question et les autres questions et entre sa réponse et les autres réponses. Toujours pour chaque question, je ne garde que les résultats supérieurs à 0.8 (valeur à modifier par la suite ?). \n",
    "\n",
    "Suite à cela, j'ai favorisé une approche plus normée. Les questions sont \"bruitées\" quand on parle de similarité : beaucoup de termes en commun ne donnant aucun indice sur la réponse à trouver. De plus, une similarité au mot semble peu utile. Il faudrait conserver **exclusivement les technolectes appartenant au champs lexical de la médecine aussi bien dans les questions que dans les réponses**. Avec ces technolectes, on peut ensuite faire de similarités au caractère, plus pertinentes selon moi dans ce contexte.\n",
    "\n",
    "Les règles de nettoyage sont les suivantes :\n",
    "\n",
    "- suppression des mots vides, de la ponctuation et des mots d'une longueur de 1. On ignore la case SANS LA SUPPRIMER pour autant.\n",
    "- on parcours une liste de mots du français (deux listes trouvées, une de 20 000 et plus occurrences, l'autre de 300 000 et plus occurrences) et pour chaque mot reconnu on l'exclu du vocabulaire des technolectes.\n",
    "- à la fin, on a une liste de mots qui ne sont pas retrouvés dans la liste de mots du français utilisée, la plupart étant des technolectes propres au champs lexical de la médecine.\n",
    "\n",
    "\n",
    "### Remarques personnelles sur la tâche\n",
    "\n",
    "Je distingue deux approches au problème :\n",
    "\n",
    "- une approche visant à savoir si le jeu de données peut se suffire afin d'obtenir de meilleurs résultats que ceux obtenus dans le papier \"source\" : X questions nous donnent Y indices sur un concept médical, donc on a des liens entre mots clefs au sein du corpus qu'on exploite par la suite.\n",
    "- une approche visant à créer ou exploiter une base de données médicale suffisement exhaustive afin de permettre une reconnaissance de termes clef entre eux : terme clef A de la question est plus souvent lié au terme B de la réponse X dans un contexte médical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40819032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports et fonctions\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import string as strii\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "sp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def removePunct(string):\n",
    "    return string.translate(str.maketrans(strii.punctuation, ' '*len(strii.punctuation))) #map punctuation to space\n",
    "\n",
    "def tokenizer(string):\n",
    "    spacy_object = sp(string)\n",
    "    return [word.text for word in spacy_object if word.is_stop == False] #and word.pos_ != \"PUNCT\" and word.pos_ != \"NUM\"\n",
    "\n",
    "def makeVoc(liste):\n",
    "    voc = []\n",
    "    for item in liste:\n",
    "        for word in tokenizer(removePunct(item)):\n",
    "            if word not in voc:\n",
    "                voc.append(word)\n",
    "    return [w for w in voc if len(w) > 1]\n",
    "\n",
    "def vectorizer(q,liste_q,ngram_range=(1,1),analyzer=\"word\"): #analyzer{‘word’, ‘char’, ‘char_wb’}\n",
    "    #vect = TfidfVectorizer()\n",
    "    vect = CountVectorizer(lowercase=True,ngram_range=ngram_range, analyzer=analyzer)\n",
    "    liste_q_vect = vect.fit_transform(liste_q) #vectoriser cet ensemble à part ? gain de temps\n",
    "    liste_q_term_matrix = liste_q_vect.toarray()\n",
    "    q_vect = vect.transform([q]).toarray()\n",
    "    return q_vect,liste_q_term_matrix\n",
    "    \n",
    "def cosinus(q_vect,liste_q_term_matrix):\n",
    "    return cosine_similarity(liste_q_term_matrix,q_vect)\n",
    "\n",
    "def writeJson(path,data):\n",
    "    with open(path,\"w\",encoding='utf-8') as f:\n",
    "        json.dump(data,f,indent=4,ensure_ascii=False)\n",
    "        \n",
    "def openJson(path):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8456032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargement du jeu de données et ajout d'une concaténation des réponses\n",
    "\n",
    "df = pd.read_csv(\"../data/csv/train.csv\",delimiter=\";\")\n",
    "\n",
    "reponses = []\n",
    "for i in range(len(df)):\n",
    "    reponses.append(\" | \".join([df[f\"answers.{l}\"][i] for l in df[\"correct_answers\"][i].split(\"|\")]))\n",
    "    \n",
    "df[\"good_answers_merge\"] = reponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "829f8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création du vocabulaire cible composé de technolectes\n",
    "\n",
    "#liste = \"liste_francais.txt\"\n",
    "liste = \"liste.de.mots.francais.frgut.txt\"\n",
    "with open(liste,'r',encoding='utf-8') as f:\n",
    "     liste_mots = [line.rstrip('\\n').lower() for line in f]\n",
    "\n",
    "vocQ = makeVoc(df[\"question\"])\n",
    "vocR = makeVoc(df[\"good_answers_merge\"])\n",
    "voc_global = set(vocQ+vocR)\n",
    "\n",
    "voc_medical = []\n",
    "for word in voc_global:\n",
    "    if word.lower() not in liste_mots:\n",
    "        voc_medical.append(word)\n",
    "\n",
    "writeJson('vocabulaire médical.json',voc_medical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11b50806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Première tâche de similarité : test en ngram char_wb\n",
    "\n",
    "voc_medical = openJson(\"vocabulaire médical.json\")\n",
    "dic = {}\n",
    "\n",
    "ids = df[\"id\"]\n",
    "questions = df[\"question\"]\n",
    "reponses = df[\"good_answers_merge\"]\n",
    "\n",
    "ids_merged_q_r = []\n",
    "merged_q_r = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    voc_m_q = [w for w in removePunct(questions[i]).split() if w in voc_medical]\n",
    "    voc_m_r = [w for w in removePunct(reponses[i]).split() if w in voc_medical]\n",
    "    if len(voc_m_q + voc_m_r) > 0:\n",
    "        merged_q_r.append((\" \".join(voc_m_q + voc_m_r)))\n",
    "        ids_merged_q_r.append(ids[i])\n",
    "\n",
    "for i,mqr in enumerate(merged_q_r):\n",
    "    dic[ids_merged_q_r[i]] = {}\n",
    "    vect_mqr,vect_merged_q_r = vectorizer(mqr,merged_q_r,ngram_range=(1,3),analyzer=\"char_wb\")\n",
    "    cos = [list(s) for s in cosinus(vect_mqr,vect_merged_q_r)]\n",
    "    for j,res in enumerate(cos):\n",
    "        dic[ids_merged_q_r[i]][ids_merged_q_r[j]] = cos[j][0]\n",
    "    \n",
    "writeJson(\"sims.json\",dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ef718a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sims.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Observation des résultats : classification des paires q/r similaires selon un seuil\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mopenJson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msims.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m similarities_classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ib, subdic \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[59], line 45\u001b[0m, in \u001b[0;36mopenJson\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopenJson\u001b[39m(path):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sims.json'"
     ]
    }
   ],
   "source": [
    "#Observation des résultats : classification des paires q/r similaires selon un seuil\n",
    "\n",
    "data = openJson(\"sims.json\")\n",
    "similarities_classes = []\n",
    "\n",
    "for ib, subdic in data.items():\n",
    "    sim_classe = []\n",
    "    for k,v in subdic.items():\n",
    "        if v > 0.8:\n",
    "            sim_classe.append(k)\n",
    "    similarities_classes.append(sim_classe)\n",
    "    \n",
    "similarities_classes_clean = []\n",
    "for l in similarities_classes:\n",
    "    if len(l) > 1:\n",
    "        similarities_classes_clean.append(l)\n",
    "    \n",
    "#on sauvegarde les clusters en supprimant les classes identiques ET LES CLASSES D'UN ELEMENT\n",
    "writeJson(\"id_classes.json\",[list(item) for item in set(tuple(row) for row in similarities_classes_clean)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73f1d71d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'id_classes.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mopenJson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid_classes.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m questions_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "Cell \u001b[0;32mIn[59], line 45\u001b[0m, in \u001b[0;36mopenJson\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopenJson\u001b[39m(path):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'id_classes.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = openJson(\"id_classes.json\")\n",
    "\n",
    "questions_list = []\n",
    "for l in data:\n",
    "    new_l = []\n",
    "    for i in range(len(df)):\n",
    "        if df[\"id\"][i] in l:\n",
    "            new_l.append((df[\"question\"][i]+\" | \"+df[\"good_answers_merge\"][i]))\n",
    "    questions_list.append(new_l)\n",
    "    \n",
    "writeJson(\"content_classes.json\",questions_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8ad7723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000000000002], [0.4076371047954098], [0.4736195135259441], [0.37956705105298494], [0.5593841424217992], [0.4033227561742197], [0.42514546110902357], [0.48656184010095194], [0.20812378052829614], [0.36244121780453775], [0.25425669046549126], [0.5459805121536903], [0.5138946247668692], [0.5116817192534651], [0.4230936828443671], [0.4434574900196697], [0.43845121490221006], [0.4281744192888377], [0.5739170359584849], [0.46913289533863944], [0.44479402718416206], [0.3723907684645208], [0.26426257915906604], [0.45457002390600576], [0.35309393180189985], [0.49788284158390317], [0.4109926471504451], [0.40664250481444153], [0.472466877391685], [0.37550739367944597], [0.3062127263296445], [0.5592589407158037], [0.5091983607108099], [0.5683725672127846], [0.4024717022032483], [0.46397758673182343], [0.5159324110497601], [0.46878577304574953], [0.3552798562032138], [0.4837814867289464], [0.3603749850782235], [0.4632862466569275], [0.43448435580964495], [0.541615063109493], [0.43148791197647496], [0.31942136563812595], [0.4150664752031365], [0.3822353935782192], [0.49823932426337475], [0.3603749850782235], [0.4794310133071944], [0.535383046784506], [0.4700910087414726], [0.3536097741979052], [0.4924148073244168], [0.49372823763033047], [0.5908946007169376], [0.4213120602120515], [0.6528410281374708], [0.5065931840054143], [0.4426338948728194], [0.4918581082237332], [0.31980107453341566], [0.5195998951469126], [0.45152938761368333], [0.42610927684772504], [0.48233989808894673], [0.4810262579090933], [0.4581240251041441], [0.5243473590806338], [0.27852424952911653], [0.30534353162371025], [0.38613264850167506], [0.39416064607362405], [0.22998382983042762], [0.41280199352159264], [0.38715518239459085], [0.4538043812286163], [0.487216977100675], [0.6157136101236393], [0.3867950227321825], [0.5383418172738443], [0.37430703378643204], [0.4798150030348475], [0.44418695701914046], [0.5386144025514555], [0.5046026401550141], [0.4534197432181285]]\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m cos \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m cosinust(vect_mqr,vect_merged_q_r)]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(cos)\n\u001b[0;32m---> 47\u001b[0m cos2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m cosinust(\u001b[43mv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmqr\u001b[49m\u001b[43m)\u001b[49m,v2(merged_q_r))]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(cos2)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j,res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cos):\n",
      "Cell \u001b[0;32mIn[67], line 6\u001b[0m, in \u001b[0;36mv1\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mv1\u001b[39m(q):\n\u001b[0;32m----> 6\u001b[0m     q_vect \u001b[38;5;241m=\u001b[39m \u001b[43mvect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_vect\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1430\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1429\u001b[0m     )\n\u001b[0;32m-> 1430\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m   1433\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, fixed_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:510\u001b[0m, in \u001b[0;36m_VectorizerMixin._check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_vocabulary()\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_:\n\u001b[0;32m--> 510\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary not fitted or provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "dft = pd.read_csv(\"../data/csv/train.csv\",delimiter=\";\")\n",
    "\n",
    "vect = CountVectorizer(lowercase=True,ngram_range=(1,3), analyzer=\"char_wb\")\n",
    "\n",
    "def v1(q):\n",
    "    q_vect = vect.transform([q]).toarray()\n",
    "    return q_vect\n",
    "\n",
    "def v2(liste_q):\n",
    "    liste_q_vect = vect.fit_transform(liste_q) #vectoriser cet ensemble à part ? gain de temps\n",
    "    liste_q_term_matrix = liste_q_vect.toarray()\n",
    "    return liste_q_term_matrix\n",
    "\n",
    "def vectorizert(q,liste_q,ngram_range=(1,1),analyzer=\"word\"): #analyzer{‘word’, ‘char’, ‘char_wb’}\n",
    "    #vect = TfidfVectorizer()\n",
    "    vect = CountVectorizer(lowercase=True,ngram_range=ngram_range, analyzer=analyzer)\n",
    "    liste_q_vect = vect.fit_transform(liste_q) #vectoriser cet ensemble à part ? gain de temps\n",
    "    liste_q_term_matrix = liste_q_vect.toarray()\n",
    "    q_vect = vect.transform([q]).toarray()\n",
    "    return q_vect,liste_q_term_matrix\n",
    "    \n",
    "def cosinust(q_vect,liste_q_term_matrix):\n",
    "    return cosine_similarity(liste_q_term_matrix,q_vect)\n",
    "\n",
    "voc_medical = openJson(\"vocabulaire médical.json\")\n",
    "dic = {}\n",
    "\n",
    "ids = df[\"id\"]\n",
    "questions = df[\"question\"]\n",
    "reponses = df[\"good_answers_merge\"]\n",
    "\n",
    "ids_merged_q_r = []\n",
    "merged_q_r = []\n",
    "\n",
    "for i in range(100):\n",
    "    voc_m_q = [w for w in removePunct(questions[i]).split() if w in voc_medical]\n",
    "    voc_m_r = [w for w in removePunct(reponses[i]).split() if w in voc_medical]\n",
    "    if len(voc_m_q + voc_m_r) > 0:\n",
    "        merged_q_r.append((\" \".join(voc_m_q + voc_m_r)))\n",
    "        ids_merged_q_r.append(ids[i])\n",
    "\n",
    "for i,mqr in enumerate(merged_q_r):\n",
    "    dic[ids_merged_q_r[i]] = {}\n",
    "    vect_mqr,vect_merged_q_r = vectorizer(mqr,merged_q_r,ngram_range=(1,3),analyzer=\"char_wb\")\n",
    "    cos = [list(s) for s in cosinust(vect_mqr,vect_merged_q_r)]\n",
    "    print(cos)\n",
    "    print()\n",
    "    cos2 = [list(s) for s in cosinust(v1(mqr),v2(merged_q_r))]\n",
    "    print(cos2)\n",
    "    for j,res in enumerate(cos):\n",
    "        dic[ids_merged_q_r[i]][ids_merged_q_r[j]] = cos[j][0]\n",
    "    \n",
    "writeJson(\"sims.json\",dic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEFT2023",
   "language": "python",
   "name": "deft2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
