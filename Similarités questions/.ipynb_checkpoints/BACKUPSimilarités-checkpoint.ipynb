{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb4a9a8",
   "metadata": {},
   "source": [
    "# Résumé de la tâche\n",
    "\n",
    "On a plusieurs éléments sur lesquels travailler à ce stade : \n",
    "- un jeu de données contenant des questions et des réponses\n",
    "- un ensemble de réponses à ces questions\n",
    "- un corpus traitant de sujets médicaux\n",
    "\n",
    "Nous disposons également de plusieurs pistes / contraintes :\n",
    "- une indication du nombre de réponses possibles dans la question (systématique ?)\n",
    "- la présence ou non d'une négation (on recherche ce qui est vrai ou ce qui est faux ?)\n",
    "- le vocabulaire médical extrait de chaque question et chaque réponse\n",
    "\n",
    "# Méthodologie\n",
    "\n",
    "La première piste serait d'extraire les informations utiles des questions, que nous avons déjà mentionnées :\n",
    "- nombre de réponses possibles\n",
    "- négation ou affirmation ?\n",
    "- vocabulaire médical\n",
    "\n",
    "Une fois ces informations extraites des questions, on peut \"unifier\" le vocabulaire de la question avec celui de chaque réponse possible à la question, créant ainsi des ensembles :\n",
    "- Q1 + R1 = E1\n",
    "- Q1 + R2 = E2\n",
    "- Q1 + R3 = E3\n",
    "- Q1 + R4 = E4\n",
    "\n",
    "Cela signifie lier un terme ou plusieurs termes médicaux (celui / ceux de la question) avec d'autres termes médicaux (ceux des réponses). Pour chaque ensemble, on regarde s'il existe dans le corpus une phrase (ou autre unité) qui lie les termes de cet ensemble grâce à une mesure de similarité en Ngrammes de char_wb. Une similarité au mot ferait peu de sens dans ce corpus en vue de la singularité des termes employés.\n",
    "\n",
    "Avec les résultats obtenus, on crée un classement. On regarde ensuite si la question contient une négation et le nombre de réponses indiquées : \n",
    "- si on a 3 résultats à trouver, on prend les trois meilleurs ensembles selon les mesures de similarité\n",
    "- si on a un résultat à trouver et une négation, on prend l'ensemble avec la pire mesure de similarité\n",
    "- si on a 2 résultats et une négation, on prend les deux pires ensembles en terme de mesure de similarité\n",
    "- ...\n",
    "\n",
    "## Extraction du vocabulaire médical\n",
    "\n",
    "Pour extraire le vocabulaire médical des questions et des réponses, plutôt que de trouver un corpus spécialisé assez exhaustif pour contenir tous les tecnolectes médicaux présents dans notre jeu de données, nous effectuons le raisonnement inverse : nous retrouvons les mots médicaux en supprimant tous les mots n'appartenant pas à ce domaine. Pour chaque question et pour chaque réponse, on a ainsi des listes de tecnolectes médicaux.\n",
    "\n",
    "## Détection de la négation (pas terminée)\n",
    "\n",
    "Il existe sûrement une solution avec Spacy (POS tagging -> étiquette \"NEG\" ?).\n",
    "Attention : il peut exister des cas où la négation est présente mais n'indique pas une recherche de résultats \"faux\". \n",
    "\n",
    "## Nombre de bons résultats attendus\n",
    "\n",
    "Voir avec Toufik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c6a7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports et fonctions\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import string as strii\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import pairwise_kernels\n",
    "\n",
    "import spacy\n",
    "sp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def removePunct(string):\n",
    "    return string.translate(str.maketrans(strii.punctuation, ' '*len(strii.punctuation))) #map punctuation to space\n",
    "\n",
    "def tokenizer(string):\n",
    "    spacy_object = sp(string)\n",
    "    return [word.text for word in spacy_object if word.is_stop == False] #and word.pos_ != \"PUNCT\" and word.pos_ != \"NUM\"\n",
    "\n",
    "def getKeywords(string,liste_mots,liste_keywords):\n",
    "    keywords = []\n",
    "    for word in tokenizer(removePunct(string)):\n",
    "            word = re.sub(\" *\",\"\",word)\n",
    "            if len(word) > 1:\n",
    "                if word.lower() not in liste_mots:\n",
    "                    keywords.append(word)\n",
    "                elif word.lower() in liste_keywords:\n",
    "                    keywords.append(word)\n",
    "    return keywords\n",
    "\n",
    "def cosine_similarity(target,liste,ngram_range=(1,1),analyzer=\"word\"):\n",
    "    vec = CountVectorizer(lowercase=True,ngram_range=ngram_range, analyzer=analyzer)\n",
    "    vec.fit(liste)\n",
    "    return pairwise_kernels(vec.transform([target]),vec.transform(liste),metric='cosine')\n",
    "\n",
    "def writeJson(path,data):\n",
    "    with open(path,\"w\",encoding='utf-8') as f:\n",
    "        json.dump(data,f,indent=4,ensure_ascii=False)\n",
    "\n",
    "def writeOutputFile(path,string):\n",
    "    with open(path, 'a',encoding='utf-8') as f:\n",
    "        f.write(f\"{string}\\n\")\n",
    "        \n",
    "def openJson(path):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nous avons déjà un corpus de référence. Afin de créer un jeu de données utilisable avec ce corpus,\n",
    "#nous extrayons de chaque question / réponse son vocabulaire\n",
    "\n",
    "liste = \"input/liste.de.mots.francais.frgut.txt\"\n",
    "with open(liste,'r',encoding='utf-8') as f:\n",
    "     liste_mots = [line.rstrip('\\n').lower() for line in f]\n",
    "liste_keywords = openJson(\"output/corpusRef/keywordsMesh.json\")\n",
    "        \n",
    "df = pd.read_csv(\"../data/csv/train.csv\",delimiter=\";\")\n",
    "questions = df[\"question\"]\n",
    "ids = df[\"id\"]\n",
    "vocByQuestions = {}\n",
    "\n",
    "for i in range(len(df)):\n",
    "    vocByQuestions[ids[i]] = []\n",
    "    keywords_q = getKeywords(questions[i],liste_mots,liste_keywords)\n",
    "    vocByQuestions[ids[i]].append(keywords_q)\n",
    "    \n",
    "    for element in [\"answers.\"+l for l in \"abcde\"]:\n",
    "        keywords = getKeywords(df[element][i],liste_mots,liste_keywords)\n",
    "        if len(keywords) < 1:\n",
    "            keywords = [\"NULL\"]\n",
    "                    \n",
    "        vocByQuestions[ids[i]].append(keywords_q+keywords)\n",
    "\n",
    "writeJson(\"output/similarités/vocByQuestions.json\",vocByQuestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openJson(\"output/similarités/vocByQuestions.json\")\n",
    "dataref = [\" \".join(liste) for liste in openJson(\"output/corpusRef/manuelMerckSentencesKeywords.json\")]\n",
    "\n",
    "results_principale = []\n",
    "results_annexe = []\n",
    "\n",
    "counter = 0\n",
    "total = len(dataset.keys())\n",
    "\n",
    "for key, listes in dataset.items():\n",
    "    res_principale = {\"id\":key,\"answers\":[]}\n",
    "    res_annexe = {\"id\":key,\"nb_answers\":0}\n",
    "    \n",
    "    counter += 1\n",
    "    print(f\"{counter}/{total}\",end=\"\\r\")\n",
    "    \n",
    "    for i,question_reponse in enumerate(listes):\n",
    "        question_reponse = \" \".join(question_reponse)\n",
    "        \n",
    "        cos = list(cosine_similarity(question_reponse,dataref)[0])\n",
    "        \n",
    "        if max(cos) >= 0.7:\n",
    "            res_principale[\"answers\"].append(str(i))\n",
    "            res_annexe[\"nb_answers\"] += 1\n",
    "            \n",
    "    results_principale.append(res_principale)\n",
    "    results_annexe.append(res_annexe)\n",
    "    \n",
    "writeJson(\"output/similarités/taskPrincipale.json\",results_principale)\n",
    "writeJson(\"output/similarités/taskAnnexe.json\",results_annexe)\n",
    "\n",
    "#garder aussi la similarité\n",
    "#pas de seuil, conserver les deux meilleures seulement parmi toutes les réponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on a trop de contenu dans le dataref, on fait un premier tri afin de créer un dataref pour chaque \n",
    "#cluster de question obtenu avec la classification des questions.\n",
    "#Ces minis datarefs ne contiennent que les elements du dataref avec au moins un mot en commun avec \n",
    "#le vocabulaire uni des questions d'un cluster.\n",
    "\n",
    "dataset = openJson(\"output/similarités/vocByQuestions.json\")\n",
    "dataref = openJson(\"output/corpusRef/manuelMerckSentencesKeywords.json\")\n",
    "question_groups = openJson(\"output/classification/id_classes.json\")\n",
    "#dataref = [\" \".join(liste) for liste in openJson(\"output/corpusRef/manuelMerckSentencesKeywords.json\")]\n",
    "\n",
    "segmented_dataref = []\n",
    "\n",
    "for group in question_groups:\n",
    "    voc = [v[0] for k,v in dataset.items() if k in group]\n",
    "    print(voc)\n",
    "    1/0\n",
    "        \n",
    "\n",
    "for key, value in dataset.items():\n",
    "    custom_dataref = []\n",
    "    \n",
    "    question_keywords = set(value[0])\n",
    "    for liste in value[1:]:\n",
    "        question_keywords.intersection_update(liste)\n",
    "    question_keywords = list(question_keywords)\n",
    "    \n",
    "    for r in dataref:\n",
    "        if len(list(set(question_keywords).intersection(r))) > 0:\n",
    "            #print(list(set(question_keywords).intersection(r)))\n",
    "            custom_dataref.append(r)\n",
    "    segmented_dataref.append(custom_dataref)\n",
    "    \n",
    "print(segmented_dataref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed319a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"pomme banane\",\"pomme\",\"banane\",\"abricot\",\"abricot banane\",\"abricot pomme\"]\n",
    "b = [\"pomme banane\",\"pomme\",\"abricot\"]\n",
    "aa = ['tt symptomatique', 'tt symptomatique', '95 normale  sériques transaminases cas cellulaires hépato taux élevés', 'traitement diagnostic Diagnostic biopsie', 'ml sulfate cas kg solution 50  magnésium hypomagnésémie IM', 'acide  repas 300 cp mg dose aspirine acétylsalicylique', 'streptobacillaire pléiomorphe bacille Streptobacillus provoquée  morsure moniliformis fièvre  Gram Fièvre rat', 'SIDA   graves chimiothérapie virus immunodéprimées muqueuse parodonte héréditaires herpès immunosuppresseurs sp Candidose infections bactérie Immunosuppression personnes maladies', ' cumulatif 50 risque', 'traits visage exposées perte soleil vésicules doigts peau zones infection']\n",
    "bb = [\"fausse particules alpha noyaux hélium\"]\n",
    "\n",
    "V = CountVectorizer(lowercase=True,ngram_range=(1,3),analyzer=\"char_wb\")\n",
    "V.fit(aa+bb)\n",
    "\n",
    "for c in bb:\n",
    "    print(pairwise_kernels(V.transform([c]),V.transform(aa),metric='cosine'))\n",
    "    \n",
    "[0.4824515401296198, 0.4824515401296198, 0.7745439610488886, 0.522860777712539, 0.7125890792026981, 0.7112519095288717, 0.642633895930556, 0.7074990394260712, 0.6226045694731197, 0.7096781511019711]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6df0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2170\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,question_reponse \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(listes):\n\u001b[1;32m     20\u001b[0m     question_reponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(question_reponse)\n\u001b[0;32m---> 22\u001b[0m     cos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_reponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataref\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(cos) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m:\n\u001b[1;32m     25\u001b[0m         good_res\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(i))\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(target, liste, ngram_range, analyzer)\u001b[0m\n\u001b[1;32m     35\u001b[0m vec \u001b[38;5;241m=\u001b[39m CountVectorizer(lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,ngram_range\u001b[38;5;241m=\u001b[39mngram_range, analyzer\u001b[38;5;241m=\u001b[39manalyzer)\n\u001b[1;32m     36\u001b[0m vec\u001b[38;5;241m.\u001b[39mfit(liste)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpairwise_kernels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliste\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:2205\u001b[0m, in \u001b[0;36mpairwise_kernels\u001b[0;34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown kernel \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m metric)\n\u001b[0;32m-> 2205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1579\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1576\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   1582\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1401\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1399\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m normalize(Y, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1401\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_normalized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/extmath.py:189\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    192\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    196\u001b[0m ):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_base.py:630\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScalar operands are not allowed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_base.py:541\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m other\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension mismatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_sparse_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# If it's a list or whatever, treat it like a matrix\u001b[39;00m\n\u001b[1;32m    544\u001b[0m other_a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(other)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_compressed.py:512\u001b[0m, in \u001b[0;36m_cs_matrix._mul_sparse_matrix\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    509\u001b[0m K2, N \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    511\u001b[0m major_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap((M, N))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 512\u001b[0m other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# convert to this format\u001b[39;00m\n\u001b[1;32m    514\u001b[0m idx_dtype \u001b[38;5;241m=\u001b[39m get_index_dtype((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindptr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices,\n\u001b[1;32m    515\u001b[0m                              other\u001b[38;5;241m.\u001b[39mindptr, other\u001b[38;5;241m.\u001b[39mindices))\n\u001b[1;32m    517\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_matmat_maxnnz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_compressed.py:33\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     31\u001b[0m         arg1 \u001b[38;5;241m=\u001b[39m arg1\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         arg1 \u001b[38;5;241m=\u001b[39m \u001b[43marg1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_self(arg1)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg1, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_base.py:376\u001b[0m, in \u001b[0;36mspmatrix.asformat\u001b[0;34m(self, format, copy)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# Forward the copy kwarg, if it's accepted.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_method()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_csc.py:140\u001b[0m, in \u001b[0;36mcsc_matrix.tocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    137\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnnz, dtype\u001b[38;5;241m=\u001b[39midx_dtype)\n\u001b[1;32m    138\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnnz, dtype\u001b[38;5;241m=\u001b[39mupcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m--> 140\u001b[0m \u001b[43mcsc_tocsr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m          \u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m          \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_csr_container(\n\u001b[1;32m    149\u001b[0m     (data, indices, indptr),\n\u001b[1;32m    150\u001b[0m     shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    152\u001b[0m A\u001b[38;5;241m.\u001b[39mhas_sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = openJson(\"output/similarités/vocByQuestions.json\")\n",
    "dataref = [\" \".join(liste) for liste in openJson(\"output/corpusRef/manuelMerckSentencesKeywords.json\")]\n",
    "\n",
    "def writeOutputFile(path,string):\n",
    "    with open(path, 'a',encoding='utf-8') as f:\n",
    "        f.write(f\"{string}\\n\")\n",
    "\n",
    "counter = 0\n",
    "total = len(dataset.keys())\n",
    "\n",
    "for key, listes in dataset.items():\n",
    "\n",
    "    good_res = []\n",
    "    nb_answers = 0\n",
    "    \n",
    "    counter += 1\n",
    "    print(f\"{counter}/{total}\",end=\"\\r\")\n",
    "    \n",
    "    for i,question_reponse in enumerate(listes):\n",
    "        question_reponse = \" \".join(question_reponse)\n",
    "        \n",
    "        cos = list(cosine_similarity(question_reponse,dataref)[0])\n",
    "\n",
    "        if max(cos) >= 0.8:\n",
    "            good_res.append(str(i))\n",
    "            nb_answers += 1\n",
    "    \n",
    "    good_res = \"|\".join(good_res).replace(\"0\",\"a\").replace(\"1\",\"b\").replace(\"2\",\"c\").replace(\"3\",\"d\").replace(\"4\",\"e\")\n",
    "    writeOutputFile(\"output/similarités/taskPrincipale.csv\",f\"{key};{good_res}\")\n",
    "    writeOutputFile(\"output/similarités/taskAnnexe.csv\",f\"{key};{nb_answers}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEFT2023",
   "language": "python",
   "name": "deft2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
