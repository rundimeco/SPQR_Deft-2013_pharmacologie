{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed4f495",
   "metadata": {},
   "source": [
    "# Tâche réalisée\n",
    "\n",
    "Ici, la tâche réalisée est la création d'un jeu de données complémentaire afin de détecter des technolectes médicaux liés.\n",
    "\n",
    "Nous avons prit l'ensemble du corpus Mesh FR et nous avons réalisé les étapes suivantes :\n",
    "- **Ouverture des colonnes jugées intéressantes du corpus** : ces colonnes contiennent une nombre important de termes médicaux, mais également des termes courants pouvant être utilisé dans le champs lexical de la médecine, et qui jusqu'alors étaient supprimés.\n",
    "- **Nettoyage des colonnes ouvertes** : on en retire la ponctuation et les mots vides afin de ne garder que les termes prévalents.\n",
    "- **Sauvegarde dans un fichier** : on conserve précieusement ce vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc07a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string as strii\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "sp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def openJson(path):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data     \n",
    "\n",
    "def writeJson(path,data):\n",
    "    with open(path,\"w\",encoding='utf-8') as f:\n",
    "        json.dump(data,f,indent=4,ensure_ascii=False)\n",
    "\n",
    "def removePunct(string):\n",
    "    return string.translate(str.maketrans(strii.punctuation, ' '*len(strii.punctuation))) #map punctuation to space\n",
    "\n",
    "def tokenizer(string):\n",
    "    sp.max_length = 10500000\n",
    "    spacy_object = sp(string, disable = ['ner', 'parser'])\n",
    "    return [word.text for word in spacy_object if word.is_stop == False] #and word.pos_ != \"PUNCT\" and word.pos_ != \"NUM\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b406f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ouverture du fichier\n",
    "\n",
    "df = pd.read_csv(\"input/MESHFRENSH.csv\",delimiter=\",\")\n",
    "\n",
    "labels = [str(l).lower() for l in df[\"Label\"]]\n",
    "synonyms = [str(s).lower() for s in df[\"Synonyms\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f543c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'une liste de mots clefs médicaux à partir de Mesh\n",
    "\n",
    "labels = tokenizer(removePunct(\" \".join(labels)))\n",
    "synonyms = tokenizer(removePunct(\" \".join(synonyms)))\n",
    "\n",
    "keywords = list(set(labels+synonyms))\n",
    "writeJson(\"output/corpusRef/keywordsMesh.json\",keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335a3244",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "writeJson() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m liste_keywords:\n\u001b[1;32m     11\u001b[0m         liste_mots_without_mesh_mots\u001b[38;5;241m.\u001b[39mappend(word)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mwriteJson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/corpusRef/mots_fr_without_mesh_words.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: writeJson() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "#Création d'une liste en épargant les termes présents dans Mesh\n",
    "\n",
    "liste = \"input/liste.de.mots.francais.frgut.txt\"\n",
    "with open(liste,'r',encoding='utf-8') as f:\n",
    "     liste_mots = [line.rstrip('\\n').lower() for line in f]\n",
    "liste_keywords = openJson(\"output/corpusRef/keywordsMesh.json\")\n",
    "\n",
    "liste_mots_without_mesh_mots = []\n",
    "for word in liste_mots:\n",
    "    if word not in liste_keywords:\n",
    "        liste_mots_without_mesh_mots.append(word)\n",
    "writeJson(\"output/corpusRef/mots_fr_without_mesh_words.json\",liste_mots_without_mesh_mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "139bd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeOutputFile(path,string):\n",
    "    with open(path, 'a',encoding='utf-8') as f:\n",
    "        f.write(f\"{string}\\n\")        \n",
    "\n",
    "for word in liste_mots_without_mesh_mots:\n",
    "    writeOutputFile(\"output/corpusRef/mots_fr_without_mesh_words.txt\",word)      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a81f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"input/corpus_corina V2(pour julien).csv\",delimiter=',')\n",
    "l = []\n",
    "answers = df[\"answer\"].dropna()\n",
    "for a in answers:\n",
    "    l.append(a)\n",
    "writeJson(\"output/Corpus_Corina.json\",l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6908997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"input/output.csv\",sep=':')\n",
    "l = []\n",
    "answers = df[\" extended response\"].dropna()\n",
    "for a in answers:\n",
    "    l.append(a.replace('\"',\"\"))\n",
    "writeJson(\"output/chatGPT.json\",l)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEFT2023",
   "language": "python",
   "name": "deft2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
